---
layout: post
title: K8S
date: 2019-03-06 09:42:31
categories: devops
share: y
excerpt_separator: <!--more-->
---


<!--more-->

## PaaS 简说
Cloud Foundry 为代表的开源 PaaS 项目，是2013年云计算中的一股清流。**PaaS 项目被大家接纳的一个主要原因，就是它提供了一种名叫“应用托管”的能力。**为了解决云端服务器和本地环境不一致的问题，PaaS开源项目应运而生。 
  
举个例子，虚拟机创建好之后，运维人员只需要在这些机器上部署一个 Cloud Foundry 项目，然后开发者只要执行一条命令就能把本地的应用部署到云上，这条命令就是：
 
```
$ cf push " 我的应用 "
```

该项目的核心组件就是一套应用的打包和分发机制。Cloud Foundry 为每种主流编程语言都定义了一种打包格式，而`cf push`的作用相当于是用户把应用的可执行文件和启动脚本打进一个压缩包内，上传到云上 Cloud Foundry 的存储中。然后，Cloud Foundry 会通过一个调度器选择一个可以运行这个应用的虚拟机，然后通知这个机器上的 Agent 把应用压缩包下载下来启动。

由于需要在一个虚拟机上启动很多个来自不同用户的应用，Cloud Foundry 会调用操作系统的 Cgroups 和 Namespace 机制为每一个应用单独创建一个称作“沙盒”的隔离环境，然后在“沙盒”中启动这些应用进程。


![../images/k8s技能图谱.jpg](../images/k8s技能图谱.jpg)

## Docker 基础

Docker 项目确实与 Cloud Foundry 的容器在大部分功能和实现原理上都是一样的。

### 为何Docker会取代PaaS

使用 PaaS， 用户就需要为每种语言、每种框架的应用维护一个打好的包。这个打包过程是没有任何章法可循的。而**docker 镜像**解决了这个问题。   

docker 镜像就是一个压缩包，但是这个压缩包不仅包含 PaaS 中的应用可执行文件 + 启停脚本。实际上，大多数 Docker 镜像是直接由一个完整操作系统的所有文件和目录构成的，也就是包含了这个应用运行所需要的所有依赖。  

docker run 创建的“沙盒”，也是使用 Cgroups 和 Namespace 机制创建出来的隔离环境。

### Linux Namespace 隔离

左图为虚拟机的工作原理，右图用一个名为Docker Engine 的软件替换了 Hypervisor。**可是这样的说法，却并不严谨。**
![docker粗略的理解](../images/docker-maybe.png)

在使用 Docker 的时候，并没有一个真正的“Docker 容器”运行在宿主机里面。Docker 项目帮助用户启动的，还是原来的应用进程，只不过在创建这些进程时，Docker 为它们加上了各种各样的 Namespace 参数。 

所以应该把 Docker 画在跟应用同级别并且靠边的位置。

![docker更准确的理解](../images/docker.jpg)

#### 隔离得不彻底

1. 多个容器之间使用的就还是同一个宿主机的操作系统内核。所以要在 Windows 宿主机上运行 Linux 容器，或者在低版本的 Linux 宿主机上运行高版本的Linux 容器，都是行不通的。
2. 在 Linux 内核中，有很多资源和对象是不能被 Namespace 化的，最典型的例子就是：时间。如果你的容器中的程序使用 settimeofday(2) 系统调用修改了时间，整个宿主机的时间都会被随之修改。

### Linux Cgroups 限制(Linux Control Group)

**限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。**

在 Linux 中，Cgroups 给用户暴露出来的操作接口是文件系统，即它以文件和目录的方式组织在操作系统的/sys/fs/cgroup 路径下。可以通过mount指令把它们展示出来：


```
$ mount -t cgroup
cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd)
cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)
cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)
cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)
cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)
cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)
cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)
cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)
cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)
cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)
```
任意打开cpu，见名知意。。。

```
# ls /sys/fs/cgroup/cpu
cgroup.clone_children  cgroup.sane_behavior  cpu.cfs_quota_us  cpu.stat      cpuacct.usage         docker      kube-proxy  notify_on_release  system.slice  user.slice
cgroup.procs           cpu.cfs_period_us     cpu.shares        cpuacct.stat  cpuacct.usage_percpu  init.scope  kubepods    release_agent      tasks
```
#### 如何使用

1. 在想要限制的资源的目录下创建一个子目录，子目录里会自动生成默认的资源配额文件。如container文件。

	`/sys/fs/cgroup/cpu# mkdir container`

2. 启动进程。
3. 修改这些文件的内容来设置限制,如
	
	```
	$ cat /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us 
	-1
	$ echo 20000 > /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us

	$ cat /sys/fs/cgroup/cpu/container/cpu.cfs_period_us 
	100000
	```
4. 把进程的PID写入container的tasks文件

	`$ echo 226 > /sys/fs/cgroup/cpu/container/tasks`

在docker中的使用：

```
$ docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash

$ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_period_us 
100000
$ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_quota_us 
20000

```

#### Cgroups 限制的不完善点

1. 如果在容器中执行top指令，会发现，它显示的信息居然是宿主机的CPU和内存数据，而不是当前容器的数据。原因就是，/proc 文件系统并不知道用户通过 Cgroups 给这个容器做了什么样的资源限制，即：/proc 文件系统不了解 Cgroups 限制的存在。

**如何解决呢？**

LXCFS是基于FUSE实现而成的一套用户态文件系统，和其他文件系统最本质的区别在于，文件系统通过用户态程序和内核FUSE模块交互完成。Linux内核从2.6.14版本开始通过FUSE模块支持在用户空间实现文件系统。通过LXCFS的源码可以看到，LXCFS主要通过调用底层fuse的lib库libfuse和内核模块fuse交互实现成一个用户态的文件系统。此外，LXCFS涉及到对cgroup文件系统的管理则是通过cgmanager用户态程序实现(为了提升性能，从0.11版本开始，LXCFS自身实现了cgfs用以替换第三方的cgroup manager,目前已经合入upstream)。


### Mount Namespace


**Mount Namespace 是对 chroot 的不断改良才被发明出来的，它也是 Linux 操作系统的第一个Namespace。**

为了能够让容器的这个根目录看起来更“真实”，我们一般会在这个容器的根目录下挂载一个完整操作系统的文件系统，比如 Ubuntu16.04 的 ISO。这样，在容器启动之后，我们在容器里通过执行"ls /" 查看根目录下的内容，，就是 Ubuntu 16.04 的所有目录和文件。

**而这个挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”。它还有一个更为专业的名字，叫作：rootfs（根文件系统）。**

对 Docker 项目来说，它最核心的原理实际上就是为待创建的用户进程：

1. 启用 Linux Namespace 配置；
2. 设置指定的 Cgroups 参数；
3. 切换进程的根目录（Change Root；优先使用pivot_root系统调用）。

**需要明确的是，rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。在 Linux 操作系统中，这两部分是分开存放的，操作系统只有在开机启动时才会加载指定版本的内核镜像。**

**正是由于 rootfs 的存在，容器才有了一个被反复宣传至今的重要特性：一致性。**

## Docker 镜像

利用联合文件系统（Union File System）的能力。联合挂载指的是比如有A,B两个目录，将它们挂载到C目录，会将A,B下的文件合并到一起，相同文件名的只会保存一份。如果在C目录下对文件进行了修改，A,B目录下的文件也会修改。

查看 Ubuntu 镜像：   
由4个层组成，每一层都是 Ubuntu 操作系统文件与目录的一部分；而在使用镜像时，Docker 会把这些增量联合挂载在一个统一的挂载点上。

```
$docker image inspect ubuntu:latest
 "RootFS": {
            "Type": "layers",
            "Layers": [
                "sha256:bebe7ce6215aee349bee5d67222abeb5c5a834bbeaa2f2f5d05363d9fd68db41",
                "sha256:283fb404ea9415ab48456fd8a82b153b1a719491cdf7b806d1853b047d00f27f",
                "sha256:663e8522d78b5b767f15b2e43885da5975068e3195bbbfa8fc3a082297a361c1",
                "sha256:4b7d93055d8781d27259ba5780938e6a78d8ef691c94ee9abc3616c1b009ec4a"
            ]
        }
```

这个挂载点就是 `/var/lib/docker/overlay2/`， 比如： 

`/var/lib/docker/overlay2/67c81fa956d4fec55e6d1113228857f5eb75d3c23acf6784cb08a9bf51d98490/merged`

```
# ls /var/lib/docker/overlay2/67c81fa956d4fec55e6d1113228857f5eb75d3c23acf6784cb08a9bf51d98490/merged
app  bin  dev  etc  home  lib  media  mnt  proc  root  run  sbin  srv  sys  tmp  usr  var
```

通过overlay2的挂载信息，我们可以找到对应的

```
# cat /proc/mounts | grep overlay2
overlay /var/lib/docker/overlay2/67c81fa956d4fec55e6d1113228857f5eb75d3c23acf6784cb08a9bf51d98490/merged overlay rw,relatime,lowerdir=/var/lib/docker/overlay2/l/LFMCDDFLSTSXU2AGLENHKE6B4B:/var/lib/docker/overlay2/l/ASBXXDHZUANZ7EJWVNXPD7GODR:/var/lib/docker/overlay2/l/4ZJE3MX4WFKTWJZPTLD2BJYWBM:/var/lib/docker/overlay2/l/JHWIGR45SLOCGTPSUHWEUC4CR6:/var/lib/docker/overlay2/l/ZTF72IYW2GRXMFLXGW5J5J27LD:/var/lib/docker/overlay2/l/3AI25CDBRO45R3OIN3HQOOU4M2,upperdir=/var/lib/docker/overlay2/67c81fa956d4fec55e6d1113228857f5eb75d3c23acf6784cb08a9bf51d98490/diff,workdir=/var/lib/docker/overlay2/67c81fa956d4fec55e6d1113228857f5eb75d3c23acf6784cb08a9bf51d98490/work 0 0
overlay /var/lib/docker/overlay2/0987b09cdd2a925038165fdad901c3208809161ff9cbf3730539a7a26a69f290/merged overlay rw,relatime,lowerdir=/var/lib/docker/overlay2/l/QRTB3LNGEMA45UAW5P3GYS5TXF:/var/lib/docker/overlay2/l/OZAXTIMJYL2Q5QOOBQ3LTYCW7C:/var/lib/docker/overlay2/l/HT2YCMJKD6C2U53UDS6X5JS47Q:/var/lib/docker/overlay2/l/5ZLH2ZYQRTFFPE5VLEW36NDWB4:/var/lib/docker/overlay2/l/PDKZRDMTL2SNQQH2I7UFE5YJSX:/var/lib/docker/overlay2/l/XA7VQQBTJQPDJVT2ABVEONDJDS:/var/lib/docker/overlay2/l/S2KJKXZU3UCVWQZZ3ATPX2SITR:/var/lib/docker/overlay2/l/RMHIDF54ZLRIEL6Z3GI4MCQ5LM,upperdir=/var/lib/docker/overlay2/0987b09cdd2a925038165fdad901c3208809161ff9cbf3730539a7a26a69f290/diff,workdir=/var/lib/docker/overlay2/0987b09cdd2a925038165fdad901c3208809161ff9cbf3730539a7a26a69f290/work 0 0
```

镜像的层都放置zai `/var/lib/docker/overlay2/l/`目录下，每层是以增量的形式分别包含了 Ubuntu 操作系统的一部分。


![](../images/docker-image.png)

**需要注意**

为了实现这样的删除操作，AuFS会在可读写层创建一个 whiteout 文件，把只读层里的文件“遮挡”起来。

#### Init层

Init 层是 Docker 项目单独生成的一个内部层，专门用来存放 /etc/hosts、/etc/resolv.conf等信息。比如在容器启动时需要写入一些指定的值比如hostname，所以就需要在可读写层对它们进行修改。可是我们并不希望执行docker commits时，把这些信息一起提交了。


## Docker 容器

docker tag 给镜像起名字时，

`$ docker tag helloworld suyue466300740/helloworld:v1
`





### docker exec 是怎么做到进入容器里的呢？

一个进程的 Namespace 信息在宿主机上是确确实实存在的，并且是以一个文件的方式存在。

通过如下指令，你可以看到当前正在运行的 Docker 容器的的进程号（PID）是 25686：

```
$ docker inspect --format '{{ .State.Pid }}'  4ddf4638572d
25686
```

这时，你可以通过查看宿主机的 proc 文件，看到这个 25686 进程的所有 Namespace对应的文件：

```
$ ls -l  /proc/25686/ns
total 0
lrwxrwxrwx 1 root root 0 Aug 13 14:05 cgroup -> cgroup:[4026531835]
lrwxrwxrwx 1 root root 0 Aug 13 14:05 ipc -> ipc:[4026532278]
lrwxrwxrwx 1 root root 0 Aug 13 14:05 mnt -> mnt:[4026532276]
lrwxrwxrwx 1 root root 0 Aug 13 14:05 net -> net:[4026532281]
lrwxrwxrwx 1 root root 0 Aug 13 14:05 pid -> pid:[4026532279]
lrwxrwxrwx 1 root root 0 Aug 13 14:05 pid_for_children -> pid:[4026532279]
lrwxrwxrwx 1 root root 0 Aug 13 14:05 user -> user:[4026531837]
lrwxrwxrwx 1 root root 0 Aug 13 14:05 uts -> uts:[4026532277]
```

一个进程，可以选择加入到某个进程已有的 Namespace当中，从而达到“进入”这个进程所在容器的目的，这正是 docker exec 的实现原理。

Docker 还专门提供了一个参数，可以让你启动一个容器并“加入”到另一个容器的 Network Namespace 里，这个参数就是 -net，比如:

```
$ docker run -it --net container:4ddf4638572d busybox ifconfig
```

而如果我指定–net=host，就意味着这个容器不会为进程启用 Network Namespace。所以，它会和宿主机上的其他普通进程一样，直接共享宿主机的网络栈。这就为容器直接操作和使用宿主机网络提供了一个渠道。

## Kubernetes

![](../images/kubernetes内部结构图.png)

**kubelet 主要负责同容器运行时（比如 Docker 项）打交道。**

1. CRI: kubelet与容器的交互通过CRI（Container Runtime Interface）的远程调用接口，这个接口定义了容器运行时的各项核心操作，比如：启动一个容器需要的所有参数。

2. OCI: 而具体的容器运行时，比如 Docker 项目，则一般通过 OCI 这个容器运行时规范同底层的Linux 操作系统进行交互，即：把 CRI 请求翻译成对 Linux 操作系统的调用（操作 Linux Namespace 和 Cgroups 等）。

3. gRPC 协议： **此外，kubelet 还通过 gRPC 协议同一个叫作 Device Plugin 的插件进行交互。**这个插件，是 Kubernetes 项目用来管理 GPU 等宿主机物理设备的主要组件，也是基于 Kubernetes 项目进行机器学习训练、高性能作业支持等工作必须关注的功能。

4. CNI（Container Networking Interface）和 CSI（Container Storage Interface）： 调用网络插件和存储插件为容器配置网络和持久化存储。

![](../images/kubernetes全景图.png)

1. Secret: 两个不同 Pod 之间要求在发起时加上授权信息。最典型的例子就是 Web 应用对数据库访问时需要 Credential（数据库的用户名和密码）信息。那么，在 Kubernetes 中这样的关系又如何处理呢？Secret 对象，它其实是一个保存在 Etcd 里的键值对数据。这样，你把 Credential 信息以 Secret 的方式存在 Etcd 里，Kubernetes 就会在你指定的 Pod（比如，Web 应用的 Pod）启动时，自动把 Secret 里的数据以 Volume 的方式挂载到容器里。这样，这个 Web 应用就可以访问数据库了。

2. **Kubernetes 定义了新的、基于 Pod 改进后的对象.**
	- job: 用来描述一次性运行的 Pod（比如，大数据任务）
	- DaemonSet: 用来描述每个宿主机上必须且只能运行一个副本的守护进程服务
	- CronJob: 则用于描述定时任务

## kubeadm

## Pod

**Pod 里的容器共享哪些 Namespace**:

1. PID Namespace(同一个pod内的容器可以看到彼此的PID)
2. Network Namespace(同一个网络空间，使用localhost访问彼此)
3. IPC Namespace(能使用SystemV IPC或者POSIX消息队列进行通信)
4. UTS Namespace(共享同一个主机名)

Pod 里的所有容器，共享的是同一个 Network Namespace，并且可以声明共享同一个 Volume。那这么来看的话，一个有 A、B 两个容器的 Pod，不就是等同于一个容器（容器 A）共享另外一个容器（容器 B）的网络和Volume，如果用docker命令实现，大概如下：

`$ docker run --net=B --volumes-from=B --name=A image-A ...`

但是这样有个问题，容器B就必须先于容器A启动，这样一个pod里的多个容器就不算对等关系，而是拓扑关系了。

所以在K8S中，pod里实现了一个中间容器，这个容器叫Infra容器。在这个pod里，Infra容器永远都是第一个被创建的容器，其他容器通过join network namespace。

![](../images/infra容器.png)

这个Infra容器使用的镜像是个特殊的镜像，k8s.gcr.io/pause。这个镜像是用一个汇编语言编写的，永远处于“暂停”状态。

**共享volume**   
Kubernetes 项目只要把所有 Volume 的定义都设计在 Pod 层级即可。

**容器设计模式 —— sidecar**

第一个最典型的例子是：WAR 包与 Web 服务器。

```
apiVersion: v1
kind: Pod
metadata:
  name: javaweb-2
spec:
  initContainers:
  - image: geektime/sample:v2
    name: war
    command: ["cp", "/sample.war", "/app"]
    volumeMounts:
    - mountPath: /app
      name: app-volume
  containers:
  - image: geektime/tomcat:7.0
    name: tomcat
    command: ["sh","-c","/root/apache-tomcat-7.0.42-v2/bin/start.sh"]
    volumeMounts:
    - mountPath: /root/apache-tomcat-7.0.42-v2/webapps
      name: app-volume
    ports:
    - containerPort: 8080
      hostPort: 8001 
  volumes:
  - name: app-volume
    emptyDir: {}

```

Init Container 容器会按顺序逐一启动，而直到它们都启动并且退出了，用户容器才会启动。

像这样，我们就用一种“组合”方式，解决了 WAR 包与 Tomcat 容器之间耦合关系的问题。

### pod 里的基本概念

如果你能把 Pod 看成传统环境里的“机器”、把容器看作是运行在这个“机器”里的“用户程序”，那么很多关于 Pod 对象的设计就非常容易理解了。

- 凡是调度、网络、存储，以及安全相关的属性，基本上是 Pod 级别的。
  1. NodeSelector：是一个供用户将 Pod 与 Node 进行绑定的字段
  2. NodeName：一旦 Pod 的这个字段被赋值，Kubernetes 项目就会被认为这个 Pod 已经经过了调度，调度的结果就是赋值的节点名字。所以，这个字段一般由调度器负责设置，但用户也可以设置它来“骗过”调度器，当然这个做法一般是在测试或者调试的时候才会用到。
  3. HostAliases：定义了 Pod 的 hosts 文件（比如 /etc/hosts）里的内容。
  
- 凡是跟容器的 Linux Namespace 相关的属性，也一定是 Pod 级别的。

   1. 
  
   ```
   apiVersion: v1
	kind: Pod
	metadata:
	  name: nginx
	spec:
	  shareProcessNamespace: true
	  containers:
	  - name: nginx
	    image: nginx
	  - name: shell
	    image: busybox
	    stdin: true
	    tty: true
   ```
   
   这个 Pod 被创建后，你就可以使用 shell 容器的 tty 跟这个容器进行交互了。
 
  `kubectl attach -it nginx -c shell`

  2. 共享宿主机的 Network、IPC 和 PID Namespace：
  
  ```
  	apiVersion: v1
	kind: Pod
	metadata:
	  name: nginx
	spec:
	  hostNetwork: true
	  hostIPC: true
	  hostPID: true
	  containers:
	  - name: nginx
	    image: nginx
	  - name: shell
	    image: busybox
	    stdin: true
	    tty: true

  ```

- Container相关

  1. ImagePullPolicy： 默认是 Always

  2. Lifecycle：Container Lifecycle Hooks

     ```
	apiVersion: v1
	kind: Pod
	metadata:
	  name: lifecycle-demo
	spec:
	  containers:
	  - name: lifecycle-demo-container
	    image: nginx
	    lifecycle:
	      postStart:
	        exec:
	          command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
	      preStop:
	        exec:
	          command: ["/usr/sbin/nginx","-s","quit"]

     ```
     
     需要明确的是，postStart 定义的操作，虽然是在 Docker 容器 ENTRYPOINT 执行之后，但它并不严格保证顺序。

### Projected Volume

在 Kubernetes 中，有几种特殊的 Volume,是为容器提供预先定义好的数据。所以，从容器的角度来看，这些 Volume 里的信息就是仿佛是被 Kubernetes“投射”（Project）进入容器当中的。

- Secret

  帮你把 Pod 想要访问的加密数据，存放到 Etcd 中。然后，你就可以通过在 Pod 的容器里挂载 Volume 的方式，访问到这些Secret里保存的信息了。比如存放数据库的Credential信息。
 
  ```
	apiVersion: v1
	kind: Pod
	metadata:
	  name: test-projected-volume 
	spec:
	  containers:
	  - name: test-secret-volume
	    image: busybox
	    args:
	    - sleep
	    - "86400"
	    volumeMounts:
	    - name: mysql-cred
	      mountPath: "/projected-volume"
	      readOnly: true
	  volumes:
	  - name: mysql-cred
	    projected:
	      sources:
	      - secret:
	          name: user
	      - secret:
	          name: pass

  ```
  Secret对象可以通过kubectl create secret指令或者通过编写YAML文件的方式来创建：
  
  ```
 	$ cat ./username.txt
	admin
	$ cat ./password.txt
	c1oudc0w!
	$ kubectl create secret generic user --from-file=./username.txt
	$ kubectl create secret generic pass --from-file=./password.txt

  ```
  或者
  
  ```
  	apiVersion: v1
	kind: Secret
	metadata:
	  name: mysecret
	type: Opaque
	data:
	  user: YWRtaW4=
	  pass: MWYyZDFlMmU2N2Rm

  ```
  Secret 对象要求这些数据必须是经过 Base64 转码的。

  
- ConfigMap

  它与 Secret 的区别在于，ConfigMap 保存的是不需要加密的、应用所需的配置信息。而 ConfigMap 的用法几乎与 Secret 完全相同。比如一个Java应用所需要的配置文件：
 
  ```
   # .properties 文件的内容
	$ cat example/ui.properties
	color.good=purple
	color.bad=yellow
	allow.textmode=true
	how.nice.to.look=fairlyNice
	
	# 从.properties 文件创建 ConfigMap
	$ kubectl create configmap ui-config --from-file=example/ui.properties
	
	# 查看这个 ConfigMap 里保存的信息 (data)
	$ kubectl get configmaps ui-config -o yaml
	apiVersion: v1
	data:
	  ui.properties: |
	    color.good=purple
	    color.bad=yellow
	    allow.textmode=true
	    how.nice.to.look=fairlyNice
	kind: ConfigMap
	metadata:
	  name: ui-config
  ...

  ```

- Downward API

  让 Pod 里的容器能够直接获取到这个 Pod API 对象本身的信息。
  
  ```
 	apiVersion: v1
	kind: Pod
	metadata:
	  name: test-downwardapi-volume
	  labels:
	    zone: us-est-coast
	    cluster: test-cluster1
	    rack: rack-22
	spec:
	  containers:
	    - name: client-container
	      image: k8s.gcr.io/busybox
	      command: ["sh", "-c"]
	      args:
	      - while true; do
	          if [[ -e /etc/podinfo/labels ]]; then
	            echo -en '\n\n'; cat /etc/podinfo/labels; fi;
	          sleep 5;
	        done;
	      volumeMounts:
	        - name: podinfo
	          mountPath: /etc/podinfo
	          readOnly: false
	  volumes:
	    - name: podinfo
	      projected:
	        sources:
	        - downwardAPI:
	            items:
	              - path: "labels"
	                fieldRef:
	                  fieldPath: metadata.labels

  ```

  目前，Downward API 支持的字段已经非常丰富了，比如:

  ```
	1. 使用 fieldRef 可以声明使用:
	spec.nodeName - 宿主机名字
	status.hostIP - 宿主机 IP
	metadata.name - Pod 的名字
	metadata.namespace - Pod 的 Namespace
	status.podIP - Pod 的 IP
	spec.serviceAccountName - Pod 的 Service Account 的名字
	metadata.uid - Pod 的 UID
	metadata.labels['<KEY>'] - 指定 <KEY> 的 Label 值
	metadata.annotations['<KEY>'] - 指定 <KEY> 的 Annotation 值
	metadata.labels - Pod 的所有 Label
	metadata.annotations - Pod 的所有 Annotation
	
	2. 使用 resourceFieldRef 可以声明使用:
	容器的 CPU limit
	容器的 CPU request
	容器的 memory limit
	容器的 memory request

  ```
  **Downward API 能够获取到的信息，一定是 Pod 里的容器进程启动之前就能够确定下来的信息。**

- ServiceAccountToken
  
  Service Account对象是K8S内置的一种“服务账户”，它是K8S进行权限分配的对象。比如，SA1只被允许对K8S API进行Get操作。像这样的授权信息就保存在它所绑定的一个特殊的Secret对象里 -- ServiceAccountToken。任何允许在K8S集群上的应用，都必须使用这个token才可以合法地访问API Server.
  
  为了方便使用，K8S提供了一个默认“服务账户”（default Service Account），并且，任何一个运行在K8S里的POD,都可以直接使用这个默认的Service Account,而无需显示地声明挂载它。
  
  ```
  	$ kubectl describe pod nginx-deployment-5c678cfb6d-lg9lw
	Containers:
	...
	  Mounts:
	    /var/run/secrets/kubernetes.io/serviceaccount from default-token-s8rbq (ro)
	Volumes:
	  default-token-s8rbq:
	  Type:       Secret (a volume populated by a Secret)
	  SecretName:  default-token-s8rbq
	  Optional:    false

  ```
  
  POD创建完成后，容器里的应用就可以直接从这个默认token的挂载目录里访问到授权信息和文件。
  
  ```
  	$ ls /var/run/secrets/kubernetes.io/serviceaccount 
	ca.crt namespace  token

  ```
  
  所以应用程序只要直接加载这个授权文件，就可以访问并操作K8S API了。而且，如果你使用的是K8S官方的Client包（k8s.io/client-go）的话，它还可以自动加载这个目录下的文件，你不需要做任何配置或者编码操作。
  
  这种把K8S 客户端以容器的方式运行在集群里，然后使用default Service Account自动授权的方式，被称为“InClusterConfig”，是被推荐的进行K8S API编程的授权方式。
  
### 容器健康检查和恢复机制

1. 方式1：

  ```
	apiVersion: v1
	kind: Pod
	metadata:
	  labels:
	    test: liveness
	  name: test-liveness-exec
	spec:
	  containers:
	  - name: liveness
	    image: busybox
	    args:
	    - /bin/sh
	    - -c
	    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
	    livenessProbe:
	      exec:
	        command:
	        - cat
	        - /tmp/healthy
	      initialDelaySeconds: 5
	      periodSeconds: 5
	
  ```

	上例中POD会保持Running状态，它会被K8S重启，这个功能就是 Kubernetes 里的Pod 恢复机制，也叫 restartPolicy。需要强调的是，Pod的恢复过程，永远都是发生在当前节点上，而不会跑到别的节点上去。事实上，一旦一个 Pod 与一个节点（Node）绑定，除非这个绑定发生了变化（pod.spec.node字段被修改），否则它永远都不会离开这个节点。这也就意味着，如果这个宿主机宕机了，这个Pod 也不会主动迁移到其他节点上去。而如果你想让 Pod 出现在其他的可用节点上，就必须使用 Deployment这样的“控制器”来管理 Pod，哪怕你只需要一个 Pod 副本。
	
	restartPolicy可能取值:
	
	- Always
	- OnFailure
	- Never

2. 方式2：

  ```
  	...
	livenessProbe:
	     httpGet:
	       path: /healthz
	       port: 8080
	       httpHeaders:
	       - name: X-Custom-Header
	         value: Awesome
	       initialDelaySeconds: 3
	       periodSeconds: 3

  ```
  
  ```
      ...
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20

  ```

### PodPreset对象

## “控制器”模型 -- Deployment 

K8S项目中的一个通用编排模式，即：控制循环，用伪代码表示如下:

```
for {
  实际状态 := 获取集群中对象 X 的实际状态（Actual State）
  期望状态 := 获取集群中对象 X 的期望状态（Desired State）
  if 实际状态 == 期望状态{
    什么都不做
  } else {
    执行编排动作，将实际状态调整为期望状态
  }
}

```


























